#!/usr/bin/env python3
"""
Project Analyzer - Ù…Ø­Ù„Ù„ Ø§Ù„Ù…Ø´Ø§Ø±ÙŠØ¹ Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠØ©
ÙŠÙ‚ÙˆÙ… Ø¨ØªØ­Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„ Ù„Ø¨Ù†ÙŠØ© ÙˆÙ…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ø´Ø§Ø±ÙŠØ¹ Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠØ©
"""

import os
import json
from pathlib import Path
from typing import Dict, List, Optional, Any
from collections import defaultdict
import logging

log = logging.getLogger(__name__)


class ProjectAnalyzer:
    """Ù…Ø­Ù„Ù„ Ø§Ù„Ù…Ø´Ø§Ø±ÙŠØ¹ - ÙŠØ­Ù„Ù„ Ø¨Ù†ÙŠØ© Ø§Ù„ÙƒÙˆØ¯ ÙˆØ§Ù„ØªØ¨Ø¹ÙŠØ§Øª ÙˆØ§Ù„Ù…Ø´Ø§ÙƒÙ„"""

    def __init__(self):
        self.supported_languages = {
            '.py': 'Python',
            '.js': 'JavaScript',
            '.ts': 'TypeScript',
            '.jsx': 'React/JSX',
            '.tsx': 'React/TSX',
            '.java': 'Java',
            '.cpp': 'C++',
            '.c': 'C',
            '.go': 'Go',
            '.rs': 'Rust',
            '.php': 'PHP',
            '.rb': 'Ruby',
            '.swift': 'Swift',
            '.kt': 'Kotlin',
            '.scala': 'Scala',
            '.sh': 'Shell',
            '.vue': 'Vue',
            '.html': 'HTML',
            '.css': 'CSS',
            '.scss': 'SCSS',
            '.sql': 'SQL',
            '.md': 'Markdown',
            '.json': 'JSON',
            '.yaml': 'YAML',
            '.yml': 'YAML',
            '.xml': 'XML',
        }

        self.config_files = {
            'package.json': 'Node.js/NPM',
            'requirements.txt': 'Python/pip',
            'Pipfile': 'Python/pipenv',
            'pyproject.toml': 'Python/Poetry',
            'setup.py': 'Python/setuptools',
            'Cargo.toml': 'Rust/Cargo',
            'pom.xml': 'Java/Maven',
            'build.gradle': 'Java/Gradle',
            'go.mod': 'Go Modules',
            'composer.json': 'PHP/Composer',
            'Gemfile': 'Ruby/Bundler',
            'docker-compose.yml': 'Docker Compose',
            'Dockerfile': 'Docker',
            '.gitignore': 'Git',
            'README.md': 'Documentation',
            'tsconfig.json': 'TypeScript',
            'webpack.config.js': 'Webpack',
            'vite.config.js': 'Vite',
        }

        self.ignore_dirs = {
            'node_modules', '__pycache__', '.git', '.venv', 'venv',
            'dist', 'build', '.pytest_cache', '.mypy_cache',
            'target', 'bin', 'obj', '.next', '.nuxt', 'out',
            'coverage', '.coverage', 'htmlcov', '.tox'
        }

        self.ignore_files = {
            '.DS_Store', 'Thumbs.db', '.gitkeep', '.env', '.env.local'
        }

    def analyze_project(self, project_path: str) -> Dict[str, Any]:
        """
        ØªØ­Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„ Ù„Ù„Ù…Ø´Ø±ÙˆØ¹

        Returns:
            Dict with analysis results including:
            - structure: Ø¨Ù†ÙŠØ© Ø§Ù„Ù…Ø´Ø±ÙˆØ¹
            - languages: Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©
            - files_stats: Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù…Ù„ÙØ§Øª
            - dependencies: Ø§Ù„ØªØ¨Ø¹ÙŠØ§Øª
            - potential_issues: Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ù…Ø­ØªÙ…Ù„Ø©
            - recommendations: ØªÙˆØµÙŠØ§Øª Ø§Ù„ØªØ­Ø³ÙŠÙ†
        """
        try:
            path = Path(project_path).resolve()

            if not path.exists():
                return {"error": f"âŒ Ø§Ù„Ù…Ø³Ø§Ø± ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯: {project_path}"}

            if not path.is_dir():
                return {"error": f"âŒ Ø§Ù„Ù…Ø³Ø§Ø± Ù„ÙŠØ³ Ù…Ø¬Ù„Ø¯Ø§Ù‹: {project_path}"}

            log.info(f"ğŸ” Ø¨Ø¯Ø¡ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹: {project_path}")

            # Ø¬Ù…Ø¹ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ø´Ø±ÙˆØ¹
            analysis = {
                "project_path": str(path),
                "project_name": path.name,
                "structure": self._analyze_structure(path),
                "languages": self._detect_languages(path),
                "files_stats": self._get_files_stats(path),
                "config_files": self._detect_config_files(path),
                "dependencies": self._analyze_dependencies(path),
                "code_metrics": self._analyze_code_metrics(path),
                "potential_issues": self._detect_issues(path),
                "recommendations": [],
            }

            # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªÙˆØµÙŠØ§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªØ­Ù„ÙŠÙ„
            analysis["recommendations"] = self._generate_recommendations(analysis)

            log.info(f"âœ… Ø§ÙƒØªÙ…Ù„ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹: {path.name}")
            return analysis

        except Exception as e:
            log.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹: {e}")
            return {"error": f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„: {str(e)}"}

    def _analyze_structure(self, path: Path) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨Ù†ÙŠØ© Ø§Ù„Ø´Ø¬Ø±ÙŠØ© Ù„Ù„Ù…Ø´Ø±ÙˆØ¹"""
        structure = {
            "total_dirs": 0,
            "total_files": 0,
            "max_depth": 0,
            "top_level_items": [],
            "main_directories": []
        }

        try:
            # Ø§Ù„Ø¹Ù†Ø§ØµØ± ÙÙŠ Ø§Ù„Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø£Ø¹Ù„Ù‰
            top_items = []
            for item in path.iterdir():
                if item.name.startswith('.') and item.name not in ['.github', '.vscode']:
                    continue
                if item.name in self.ignore_dirs:
                    continue

                top_items.append({
                    "name": item.name,
                    "type": "dir" if item.is_dir() else "file",
                    "size": self._get_size(item)
                })

            structure["top_level_items"] = sorted(top_items,
                                                 key=lambda x: (x["type"] != "dir", x["name"]))

            # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¹Ù…Ù‚ ÙˆØ§Ù„Ù…Ù„ÙØ§Øª
            for root, dirs, files in os.walk(path):
                # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª Ø§Ù„Ù…Ø³ØªØ¨Ø¹Ø¯Ø©
                dirs[:] = [d for d in dirs if d not in self.ignore_dirs]

                depth = len(Path(root).relative_to(path).parts)
                structure["max_depth"] = max(structure["max_depth"], depth)
                structure["total_dirs"] += len(dirs)
                structure["total_files"] += len([f for f in files
                                                if f not in self.ignore_files])

                # Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© (ÙÙŠ Ø§Ù„Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø£ÙˆÙ„ ÙÙ‚Ø·)
                if depth == 1:
                    for d in dirs:
                        dir_path = Path(root) / d
                        structure["main_directories"].append({
                            "name": d,
                            "size": self._get_size(dir_path),
                            "files_count": sum(1 for _ in dir_path.rglob("*") if _.is_file())
                        })

            return structure

        except Exception as e:
            log.error(f"Error analyzing structure: {e}")
            return structure

    def _detect_languages(self, path: Path) -> Dict[str, Any]:
        """ÙƒØ´Ù Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠØ© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©"""
        languages = defaultdict(lambda: {"count": 0, "lines": 0, "size": 0})

        try:
            for ext, lang in self.supported_languages.items():
                files = list(path.rglob(f"*{ext}"))
                # ØªØµÙÙŠØ© Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª Ø§Ù„Ù…Ø³ØªØ¨Ø¹Ø¯Ø©
                files = [f for f in files
                        if not any(ignored in f.parts for ignored in self.ignore_dirs)]

                if files:
                    total_lines = 0
                    total_size = 0

                    for file in files:
                        try:
                            total_size += file.stat().st_size
                            # Ø¹Ø¯ Ø§Ù„Ø£Ø³Ø·Ø± Ù„Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù†ØµÙŠØ©
                            if ext not in ['.json', '.xml']:
                                with open(file, 'r', encoding='utf-8', errors='ignore') as f:
                                    total_lines += sum(1 for _ in f)
                        except:
                            pass

                    languages[lang]["count"] = len(files)
                    languages[lang]["lines"] = total_lines
                    languages[lang]["size"] = total_size

            # ØªØ±ØªÙŠØ¨ Ø­Ø³Ø¨ Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ù„ÙØ§Øª
            sorted_langs = dict(sorted(languages.items(),
                                     key=lambda x: x[1]["count"],
                                     reverse=True))

            return {
                "detected": list(sorted_langs.keys()),
                "details": sorted_langs,
                "primary": list(sorted_langs.keys())[0] if sorted_langs else "Unknown"
            }

        except Exception as e:
            log.error(f"Error detecting languages: {e}")
            return {"detected": [], "details": {}, "primary": "Unknown"}

    def _get_files_stats(self, path: Path) -> Dict[str, Any]:
        """Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù…Ù„ÙØ§Øª"""
        stats = {
            "total_size": 0,
            "total_files": 0,
            "largest_files": [],
            "file_types": defaultdict(int)
        }

        try:
            files_info = []

            for file in path.rglob("*"):
                if not file.is_file():
                    continue
                if any(ignored in file.parts for ignored in self.ignore_dirs):
                    continue
                if file.name in self.ignore_files:
                    continue

                try:
                    size = file.stat().st_size
                    stats["total_size"] += size
                    stats["total_files"] += 1
                    stats["file_types"][file.suffix or "no_ext"] += 1

                    files_info.append({
                        "path": str(file.relative_to(path)),
                        "size": size,
                        "size_str": self._format_size(size)
                    })
                except:
                    pass

            # Ø£ÙƒØ¨Ø± 10 Ù…Ù„ÙØ§Øª
            stats["largest_files"] = sorted(files_info,
                                          key=lambda x: x["size"],
                                          reverse=True)[:10]

            stats["total_size_str"] = self._format_size(stats["total_size"])

            return stats

        except Exception as e:
            log.error(f"Error getting files stats: {e}")
            return stats

    def _detect_config_files(self, path: Path) -> List[Dict[str, str]]:
        """ÙƒØ´Ù Ù…Ù„ÙØ§Øª Ø§Ù„ØªÙƒÙˆÙŠÙ† Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø©"""
        found_configs = []

        try:
            for config_name, config_type in self.config_files.items():
                config_path = path / config_name
                if config_path.exists():
                    found_configs.append({
                        "file": config_name,
                        "type": config_type,
                        "path": str(config_path.relative_to(path))
                    })

            return found_configs

        except Exception as e:
            log.error(f"Error detecting config files: {e}")
            return found_configs

    def _analyze_dependencies(self, path: Path) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØ¨Ø¹ÙŠØ§Øª Ù…Ù† Ù…Ù„ÙØ§Øª Ø§Ù„ØªÙƒÙˆÙŠÙ†"""
        dependencies = {
            "python": [],
            "node": [],
            "other": []
        }

        try:
            # Python - requirements.txt
            req_file = path / "requirements.txt"
            if req_file.exists():
                with open(req_file, 'r', encoding='utf-8') as f:
                    deps = [line.strip() for line in f if line.strip() and not line.startswith('#')]
                    dependencies["python"] = deps[:20]  # Ø£ÙˆÙ„ 20 ØªØ¨Ø¹ÙŠØ©

            # Python - pyproject.toml
            pyproject_file = path / "pyproject.toml"
            if pyproject_file.exists():
                try:
                    with open(pyproject_file, 'r', encoding='utf-8') as f:
                        content = f.read()
                        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨Ø³ÙŠØ· Ù„Ù„ØªØ¨Ø¹ÙŠØ§Øª
                        if '[tool.poetry.dependencies]' in content:
                            dependencies["python"].append("(Poetry dependencies found)")
                except:
                    pass

            # Node.js - package.json
            package_file = path / "package.json"
            if package_file.exists():
                try:
                    with open(package_file, 'r', encoding='utf-8') as f:
                        package_data = json.load(f)

                        deps = package_data.get("dependencies", {})
                        dev_deps = package_data.get("devDependencies", {})

                        dependencies["node"] = {
                            "dependencies": list(deps.keys())[:15],
                            "devDependencies": list(dev_deps.keys())[:15],
                            "total": len(deps) + len(dev_deps)
                        }
                except:
                    pass

            return dependencies

        except Exception as e:
            log.error(f"Error analyzing dependencies: {e}")
            return dependencies

    def _analyze_code_metrics(self, path: Path) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„ÙƒÙˆØ¯"""
        metrics = {
            "total_lines": 0,
            "code_lines": 0,
            "comment_lines": 0,
            "blank_lines": 0,
            "avg_file_size": 0
        }

        try:
            code_files = []
            for ext in ['.py', '.js', '.ts', '.jsx', '.tsx', '.java', '.cpp', '.go', '.rs']:
                code_files.extend(path.rglob(f"*{ext}"))

            # ØªØµÙÙŠØ© Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø³ØªØ¨Ø¹Ø¯Ø©
            code_files = [f for f in code_files
                         if not any(ignored in f.parts for ignored in self.ignore_dirs)]

            if code_files:
                total_size = 0
                for file in code_files[:100]:  # ØªØ­Ù„ÙŠÙ„ Ø£ÙˆÙ„ 100 Ù…Ù„Ù ÙÙ‚Ø· Ù„Ù„Ø³Ø±Ø¹Ø©
                    try:
                        with open(file, 'r', encoding='utf-8', errors='ignore') as f:
                            for line in f:
                                metrics["total_lines"] += 1
                                stripped = line.strip()
                                if not stripped:
                                    metrics["blank_lines"] += 1
                                elif stripped.startswith('#') or stripped.startswith('//'):
                                    metrics["comment_lines"] += 1
                                else:
                                    metrics["code_lines"] += 1

                        total_size += file.stat().st_size
                    except:
                        pass

                metrics["avg_file_size"] = total_size // len(code_files) if code_files else 0
                metrics["files_analyzed"] = len(code_files[:100])
                metrics["total_code_files"] = len(code_files)

            return metrics

        except Exception as e:
            log.error(f"Error analyzing code metrics: {e}")
            return metrics

    def _detect_issues(self, path: Path) -> List[Dict[str, str]]:
        """ÙƒØ´Ù Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ù…Ø­ØªÙ…Ù„Ø© ÙÙŠ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹"""
        issues = []

        try:
            # ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ README
            if not (path / "README.md").exists() and not (path / "README").exists():
                issues.append({
                    "type": "documentation",
                    "severity": "medium",
                    "message": "Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù…Ù„Ù README - ÙŠÙØ¶Ù„ Ø¥Ø¶Ø§ÙØ© ØªÙˆØ«ÙŠÙ‚ Ù„Ù„Ù…Ø´Ø±ÙˆØ¹"
                })

            # ØªØ­Ù‚Ù‚ Ù…Ù† .gitignore
            if not (path / ".gitignore").exists():
                issues.append({
                    "type": "version_control",
                    "severity": "low",
                    "message": "Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù…Ù„Ù .gitignore - Ù‚Ø¯ ÙŠØªÙ… Ø±ÙØ¹ Ù…Ù„ÙØ§Øª ØºÙŠØ± Ù…Ø±ØºÙˆØ¨Ø©"
                })

            # ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ tests
            test_dirs = ['tests', 'test', '__tests__', 'spec']
            has_tests = any((path / test_dir).exists() for test_dir in test_dirs)
            if not has_tests:
                issues.append({
                    "type": "testing",
                    "severity": "medium",
                    "message": "Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø¬Ù„Ø¯Ø§Øª Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª - ÙŠÙØ¶Ù„ Ø¥Ø¶Ø§ÙØ© Unit Tests"
                })

            # ØªØ­Ù‚Ù‚ Ù…Ù† Ù…Ù„ÙØ§Øª ÙƒØ¨ÙŠØ±Ø© Ø¬Ø¯Ø§Ù‹
            for file in path.rglob("*"):
                if not file.is_file():
                    continue
                if any(ignored in file.parts for ignored in self.ignore_dirs):
                    continue

                try:
                    size = file.stat().st_size
                    # Ù…Ù„Ù Ø£ÙƒØ¨Ø± Ù…Ù† 5MB
                    if size > 5 * 1024 * 1024:
                        issues.append({
                            "type": "file_size",
                            "severity": "low",
                            "message": f"Ù…Ù„Ù ÙƒØ¨ÙŠØ±: {file.name} ({self._format_size(size)})"
                        })
                except:
                    pass

            return issues[:10]  # Ø£ÙˆÙ„ 10 Ù…Ø´Ø§ÙƒÙ„ ÙÙ‚Ø·

        except Exception as e:
            log.error(f"Error detecting issues: {e}")
            return issues

    def _generate_recommendations(self, analysis: Dict[str, Any]) -> List[str]:
        """ØªÙˆÙ„ÙŠØ¯ ØªÙˆØµÙŠØ§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªØ­Ù„ÙŠÙ„"""
        recommendations = []

        try:
            # ØªÙˆØµÙŠØ§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù„ØºØ§Øª
            primary_lang = analysis["languages"].get("primary", "")

            if primary_lang == "Python":
                if not any(c["type"] == "Python/pip" for c in analysis["config_files"]):
                    recommendations.append("ğŸ’¡ ÙŠÙØ¶Ù„ Ø¥Ø¶Ø§ÙØ© Ù…Ù„Ù requirements.txt Ù„ØªÙˆØ«ÙŠÙ‚ Ø§Ù„ØªØ¨Ø¹ÙŠØ§Øª")

                if not any(c["file"] == ".gitignore" for c in analysis["config_files"]):
                    recommendations.append("ğŸ’¡ Ø£Ø¶Ù .gitignore Ù„Ø§Ø³ØªØ¨Ø¹Ø§Ø¯ __pycache__ Ùˆ .venv")

            elif primary_lang in ["JavaScript", "TypeScript"]:
                if not any(c["type"] == "Node.js/NPM" for c in analysis["config_files"]):
                    recommendations.append("ğŸ’¡ ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ package.json Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­")

            # ØªÙˆØµÙŠØ§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø­Ø¬Ù…
            if analysis["files_stats"]["total_files"] > 1000:
                recommendations.append("ğŸ“Š Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ ÙƒØ¨ÙŠØ± - ÙÙƒØ± ÙÙŠ ØªÙ‚Ø³ÙŠÙ…Ù‡ Ø¥Ù„Ù‰ modules Ù…Ù†ÙØµÙ„Ø©")

            # ØªÙˆØµÙŠØ§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø´Ø§ÙƒÙ„
            if len(analysis["potential_issues"]) > 5:
                recommendations.append("âš ï¸  ÙŠÙˆØ¬Ø¯ Ø¹Ø¯Ø© Ù…Ø´Ø§ÙƒÙ„ Ù…Ø­ØªÙ…Ù„Ø© - Ø±Ø§Ø¬Ø¹ Ù‚Ø³Ù… Ø§Ù„Ù…Ø´Ø§ÙƒÙ„")

            # ØªÙˆØµÙŠØ§Øª Ø¹Ø§Ù…Ø©
            if analysis["structure"]["max_depth"] > 7:
                recommendations.append("ğŸ”„ Ø§Ù„Ø¨Ù†ÙŠØ© Ø¹Ù…ÙŠÙ‚Ø© Ø¬Ø¯Ø§Ù‹ - ÙÙƒØ± ÙÙŠ ØªØ³Ø·ÙŠØ­ Hierarchy")

            if not recommendations:
                recommendations.append("âœ… Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ ÙŠØ¨Ø¯Ùˆ Ù…Ù†Ø¸Ù…Ø§Ù‹ Ø¨Ø´ÙƒÙ„ Ø¬ÙŠØ¯!")

            return recommendations

        except Exception as e:
            log.error(f"Error generating recommendations: {e}")
            return ["Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªÙˆØµÙŠØ§Øª"]

    def _get_size(self, path: Path) -> int:
        """Ø­Ø³Ø§Ø¨ Ø§Ù„Ø­Ø¬Ù… Ø§Ù„ÙƒÙ„ÙŠ Ù„Ù…Ù„Ù Ø£Ùˆ Ù…Ø¬Ù„Ø¯"""
        if path.is_file():
            return path.stat().st_size

        total = 0
        try:
            for item in path.rglob("*"):
                if item.is_file():
                    try:
                        total += item.stat().st_size
                    except:
                        pass
        except:
            pass

        return total

    def _format_size(self, size: int) -> str:
        """ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ø­Ø¬Ù… Ø¨Ø´ÙƒÙ„ Ù…Ù‚Ø±ÙˆØ¡"""
        for unit in ['B', 'KB', 'MB', 'GB']:
            if size < 1024.0:
                return f"{size:.1f} {unit}"
            size /= 1024.0
        return f"{size:.1f} TB"

    def generate_report(self, analysis: Dict[str, Any]) -> str:
        """ØªÙˆÙ„ÙŠØ¯ ØªÙ‚Ø±ÙŠØ± Ù†ØµÙŠ Ø´Ø§Ù…Ù„ Ù…Ù† Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ­Ù„ÙŠÙ„"""
        if "error" in analysis:
            return f"âŒ Ø®Ø·Ø£: {analysis['error']}"

        report_lines = []

        # Ø§Ù„Ø¹Ù†ÙˆØ§Ù†
        report_lines.append("=" * 70)
        report_lines.append(f"ğŸ“Š ØªÙ‚Ø±ÙŠØ± ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹: {analysis['project_name']}")
        report_lines.append("=" * 70)
        report_lines.append("")

        # Ø§Ù„Ø¨Ù†ÙŠØ©
        struct = analysis["structure"]
        report_lines.append("ğŸ“ Ø¨Ù†ÙŠØ© Ø§Ù„Ù…Ø´Ø±ÙˆØ¹:")
        report_lines.append(f"   â€¢ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª: {struct['total_dirs']}")
        report_lines.append(f"   â€¢ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…Ù„ÙØ§Øª: {struct['total_files']}")
        report_lines.append(f"   â€¢ Ø£Ù‚ØµÙ‰ Ø¹Ù…Ù‚: {struct['max_depth']}")
        report_lines.append("")

        # Ø§Ù„Ù„ØºØ§Øª
        langs = analysis["languages"]
        report_lines.append("ğŸ’» Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ù…ÙƒØªØ´ÙØ©:")
        report_lines.append(f"   â€¢ Ø§Ù„Ù„ØºØ© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©: {langs['primary']}")
        for lang, details in list(langs["details"].items())[:5]:
            report_lines.append(f"   â€¢ {lang}: {details['count']} Ù…Ù„ÙØŒ {details['lines']:,} Ø³Ø·Ø±")
        report_lines.append("")

        # Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
        stats = analysis["files_stats"]
        report_lines.append("ğŸ“ˆ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª:")
        report_lines.append(f"   â€¢ Ø§Ù„Ø­Ø¬Ù… Ø§Ù„ÙƒÙ„ÙŠ: {stats['total_size_str']}")
        report_lines.append(f"   â€¢ Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ù„ÙØ§Øª: {stats['total_files']}")
        report_lines.append("")

        # Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„ÙƒÙˆØ¯
        metrics = analysis["code_metrics"]
        if metrics["total_lines"] > 0:
            report_lines.append("ğŸ“ Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„ÙƒÙˆØ¯:")
            report_lines.append(f"   â€¢ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø£Ø³Ø·Ø±: {metrics['total_lines']:,}")
            report_lines.append(f"   â€¢ Ø£Ø³Ø·Ø± Ø§Ù„ÙƒÙˆØ¯: {metrics['code_lines']:,}")
            report_lines.append(f"   â€¢ Ø£Ø³Ø·Ø± Ø§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª: {metrics['comment_lines']:,}")
            report_lines.append(f"   â€¢ Ø£Ø³Ø·Ø± ÙØ§Ø±ØºØ©: {metrics['blank_lines']:,}")
            report_lines.append("")

        # Ø§Ù„ØªØ¨Ø¹ÙŠØ§Øª
        deps = analysis["dependencies"]
        if deps["python"] or deps["node"]:
            report_lines.append("ğŸ“¦ Ø§Ù„ØªØ¨Ø¹ÙŠØ§Øª:")
            if deps["python"]:
                report_lines.append(f"   â€¢ Python: {len(deps['python'])} ØªØ¨Ø¹ÙŠØ©")
            if deps["node"]:
                if isinstance(deps["node"], dict):
                    report_lines.append(f"   â€¢ Node.js: {deps['node'].get('total', 0)} ØªØ¨Ø¹ÙŠØ©")
            report_lines.append("")

        # Ø§Ù„Ù…Ø´Ø§ÙƒÙ„
        issues = analysis["potential_issues"]
        if issues:
            report_lines.append("âš ï¸  Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ù…Ø­ØªÙ…Ù„Ø©:")
            for issue in issues[:5]:
                report_lines.append(f"   â€¢ [{issue['severity']}] {issue['message']}")
            report_lines.append("")

        # Ø§Ù„ØªÙˆØµÙŠØ§Øª
        recommendations = analysis["recommendations"]
        if recommendations:
            report_lines.append("ğŸ’¡ Ø§Ù„ØªÙˆØµÙŠØ§Øª:")
            for rec in recommendations:
                report_lines.append(f"   {rec}")
            report_lines.append("")

        report_lines.append("=" * 70)

        return "\n".join(report_lines)


# Instance Ø¹Ø§Ù…Ø©
project_analyzer = ProjectAnalyzer()


if __name__ == "__main__":
    # Ø§Ø®ØªØ¨Ø§Ø±
    import sys

    if len(sys.argv) > 1:
        project_path = sys.argv[1]
    else:
        project_path = "."

    print(f"ğŸ” ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹: {project_path}\n")

    analysis = project_analyzer.analyze_project(project_path)
    report = project_analyzer.generate_report(analysis)

    print(report)
