"""
Meta-Confidence Calibration System - Ù†Ø¸Ø§Ù… Ù…Ø¹Ø§ÙŠØ±Ø© Ø§Ù„Ø«Ù‚Ø© Ø§Ù„Ø°Ø§ØªÙŠØ©
==================================================================

ÙŠÙ‚ÙŠØ³ Ø«Ù‚Ø© Ø§Ù„Ù†Ø¸Ø§Ù… ÙÙŠ Ù‚Ø±Ø§Ø±Ø§ØªÙ‡ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¹ÙˆØ§Ù…Ù„ Ù…ØªØ¹Ø¯Ø¯Ø© ÙˆÙŠÙÙ†ØªØ¬
Certainty Index Ù„ÙƒÙ„ Ù…Ù‡Ù…Ø©

Addresses Deep Cognition Q16 (Meta-confidence calibration)

Author: Noogh AI Team
Date: 2025-11-10
Priority: CRITICAL
"""

from typing import Dict, Any, List, Optional, Tuple
from dataclasses import dataclass, field
from datetime import datetime, timezone
from enum import Enum
import math


class CertaintyLevel(str, Enum):
    """Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ÙŠÙ‚ÙŠÙ†"""
    ABSOLUTE = "absolute"  # 95%+ - ÙŠÙ‚ÙŠÙ† Ù…Ø·Ù„Ù‚
    HIGH = "high"  # 85-94% - Ø«Ù‚Ø© Ø¹Ø§Ù„ÙŠØ©
    MODERATE = "moderate"  # 70-84% - Ø«Ù‚Ø© Ù…ØªÙˆØ³Ø·Ø©
    LOW = "low"  # 50-69% - Ø«Ù‚Ø© Ù…Ù†Ø®ÙØ¶Ø©
    UNCERTAIN = "uncertain"  # 30-49% - ØºÙŠØ± Ù…ØªØ£ÙƒØ¯
    VERY_UNCERTAIN = "very_uncertain"  # 0-29% - Ø´Ùƒ ÙƒØ¨ÙŠØ±


class ConfidenceSource(str, Enum):
    """Ù…ØµØ¯Ø± Ø§Ù„Ø«Ù‚Ø©"""
    DATA_QUALITY = "data_quality"  # Ø¬ÙˆØ¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    MODEL_AGREEMENT = "model_agreement"  # ØªÙˆØ§ÙÙ‚ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
    HISTORICAL_ACCURACY = "historical_accuracy"  # Ø§Ù„Ø¯Ù‚Ø© Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠØ©
    CONTEXT_CLARITY = "context_clarity"  # ÙˆØ¶ÙˆØ­ Ø§Ù„Ø³ÙŠØ§Ù‚
    REASONING_DEPTH = "reasoning_depth"  # Ø¹Ù…Ù‚ Ø§Ù„ØªÙÙƒÙŠØ±
    CROSS_VALIDATION = "cross_validation"  # Ø§Ù„ØªØ­Ù‚Ù‚ Ø§Ù„Ù…ØªÙ‚Ø§Ø·Ø¹


@dataclass
class ConfidenceFactor:
    """Ø¹Ø§Ù…Ù„ Ø«Ù‚Ø© ÙˆØ§Ø­Ø¯"""
    source: ConfidenceSource
    score: float  # 0.0-1.0
    weight: float  # 0.0-1.0
    evidence: str


@dataclass
class CertaintyIndex:
    """Ù…Ø¤Ø´Ø± Ø§Ù„ÙŠÙ‚ÙŠÙ† Ø§Ù„ÙƒØ§Ù…Ù„"""
    overall_confidence: float  # 0.0-1.0
    certainty_level: CertaintyLevel
    factors: List[ConfidenceFactor]
    calibration_quality: float  # Ù…Ø¯Ù‰ Ø¯Ù‚Ø© Ø§Ù„Ù…Ø¹Ø§ÙŠØ±Ø©
    recommendation: str
    timestamp: datetime = field(default_factory=lambda: datetime.now(timezone.utc))

    def to_dict(self) -> Dict[str, Any]:
        return {
            "overall_confidence": self.overall_confidence,
            "certainty_level": self.certainty_level.value,
            "factors": [
                {
                    "source": f.source.value,
                    "score": f.score,
                    "weight": f.weight,
                    "evidence": f.evidence
                }
                for f in self.factors
            ],
            "calibration_quality": self.calibration_quality,
            "recommendation": self.recommendation,
            "timestamp": self.timestamp.isoformat()
        }


class MetaConfidenceCalibrator:
    """
    Ù…Ø¹Ø§ÙŠÙØ± Ø§Ù„Ø«Ù‚Ø© Ø§Ù„Ø°Ø§ØªÙŠØ©

    ÙŠØ­Ø³Ø¨ Ù…Ø¤Ø´Ø± Ø§Ù„ÙŠÙ‚ÙŠÙ† (Certainty Index) Ù„ÙƒÙ„ Ù‚Ø±Ø§Ø± Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰:
    1. Ø¬ÙˆØ¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¯Ø®Ù„Ø©
    2. ØªÙˆØ§ÙÙ‚ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø®ØªÙ„ÙØ©
    3. Ø§Ù„Ø¯Ù‚Ø© Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠØ©
    4. ÙˆØ¶ÙˆØ­ Ø§Ù„Ø³ÙŠØ§Ù‚
    5. Ø¹Ù…Ù‚ Ø§Ù„ØªÙÙƒÙŠØ±
    6. Ø§Ù„ØªØ­Ù‚Ù‚ Ø§Ù„Ù…ØªÙ‚Ø§Ø·Ø¹
    """

    def __init__(self):
        # Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ø¹ÙˆØ§Ù…Ù„ Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©
        self.default_weights = {
            ConfidenceSource.DATA_QUALITY: 0.25,
            ConfidenceSource.MODEL_AGREEMENT: 0.20,
            ConfidenceSource.HISTORICAL_ACCURACY: 0.15,
            ConfidenceSource.CONTEXT_CLARITY: 0.15,
            ConfidenceSource.REASONING_DEPTH: 0.15,
            ConfidenceSource.CROSS_VALIDATION: 0.10
        }

        # Ø³Ø¬Ù„ Ø§Ù„Ù‚Ø±Ø§Ø±Ø§Øª Ø§Ù„Ø³Ø§Ø¨Ù‚Ø© (Ù„Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ø£Ø®Ø·Ø§Ø¡)
        self.decision_history: List[Dict[str, Any]] = []

        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù…Ø¹Ø§ÙŠØ±Ø©
        self.total_decisions = 0
        self.calibration_errors: List[float] = []  # ÙØ±Ù‚ Ø¨ÙŠÙ† Ø§Ù„Ø«Ù‚Ø© Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø© ÙˆØ§Ù„ÙØ¹Ù„ÙŠØ©

    def calculate_certainty(self,
                           data_quality: Optional[float] = None,
                           model_agreement: Optional[float] = None,
                           historical_accuracy: Optional[float] = None,
                           context_clarity: Optional[float] = None,
                           reasoning_depth: Optional[float] = None,
                           cross_validation: Optional[float] = None,
                           custom_weights: Optional[Dict[ConfidenceSource, float]] = None) -> CertaintyIndex:
        """
        Ø­Ø³Ø§Ø¨ Ù…Ø¤Ø´Ø± Ø§Ù„ÙŠÙ‚ÙŠÙ†

        Args:
            data_quality: Ø¬ÙˆØ¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (0.0-1.0)
            model_agreement: ØªÙˆØ§ÙÙ‚ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ (0.0-1.0)
            historical_accuracy: Ø§Ù„Ø¯Ù‚Ø© Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠØ© (0.0-1.0)
            context_clarity: ÙˆØ¶ÙˆØ­ Ø§Ù„Ø³ÙŠØ§Ù‚ (0.0-1.0)
            reasoning_depth: Ø¹Ù…Ù‚ Ø§Ù„ØªÙÙƒÙŠØ± (0.0-1.0)
            cross_validation: Ø§Ù„ØªØ­Ù‚Ù‚ Ø§Ù„Ù…ØªÙ‚Ø§Ø·Ø¹ (0.0-1.0)
            custom_weights: Ø£ÙˆØ²Ø§Ù† Ù…Ø®ØµØµØ© (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)

        Returns:
            CertaintyIndex
        """
        weights = custom_weights or self.default_weights

        # Ø¬Ù…Ø¹ Ø§Ù„Ø¹ÙˆØ§Ù…Ù„ Ø§Ù„Ù…ØªÙˆÙØ±Ø©
        factors: List[ConfidenceFactor] = []

        if data_quality is not None:
            factors.append(ConfidenceFactor(
                source=ConfidenceSource.DATA_QUALITY,
                score=data_quality,
                weight=weights[ConfidenceSource.DATA_QUALITY],
                evidence=self._get_data_quality_evidence(data_quality)
            ))

        if model_agreement is not None:
            factors.append(ConfidenceFactor(
                source=ConfidenceSource.MODEL_AGREEMENT,
                score=model_agreement,
                weight=weights[ConfidenceSource.MODEL_AGREEMENT],
                evidence=self._get_model_agreement_evidence(model_agreement)
            ))

        if historical_accuracy is not None:
            factors.append(ConfidenceFactor(
                source=ConfidenceSource.HISTORICAL_ACCURACY,
                score=historical_accuracy,
                weight=weights[ConfidenceSource.HISTORICAL_ACCURACY],
                evidence=self._get_historical_accuracy_evidence(historical_accuracy)
            ))

        if context_clarity is not None:
            factors.append(ConfidenceFactor(
                source=ConfidenceSource.CONTEXT_CLARITY,
                score=context_clarity,
                weight=weights[ConfidenceSource.CONTEXT_CLARITY],
                evidence=self._get_context_clarity_evidence(context_clarity)
            ))

        if reasoning_depth is not None:
            factors.append(ConfidenceFactor(
                source=ConfidenceSource.REASONING_DEPTH,
                score=reasoning_depth,
                weight=weights[ConfidenceSource.REASONING_DEPTH],
                evidence=self._get_reasoning_depth_evidence(reasoning_depth)
            ))

        if cross_validation is not None:
            factors.append(ConfidenceFactor(
                source=ConfidenceSource.CROSS_VALIDATION,
                score=cross_validation,
                weight=weights[ConfidenceSource.CROSS_VALIDATION],
                evidence=self._get_cross_validation_evidence(cross_validation)
            ))

        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø«Ù‚Ø© Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠØ© (weighted average)
        if factors:
            total_weight = sum(f.weight for f in factors)
            overall_confidence = sum(f.score * f.weight for f in factors) / total_weight if total_weight > 0 else 0.0
        else:
            overall_confidence = 0.5  # Ù‚ÙŠÙ…Ø© Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¹Ù†Ø¯ Ø¹Ø¯Ù… ØªÙˆÙØ± Ø¨ÙŠØ§Ù†Ø§Øª

        # ØªØ·Ø¨ÙŠÙ‚ Ù…Ø¹Ø§ÙŠØ±Ø© Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠØ© (ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©)
        calibrated_confidence = self._apply_calibration(overall_confidence, factors)

        # ØªØ­Ø¯ÙŠØ¯ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ÙŠÙ‚ÙŠÙ†
        certainty_level = self._determine_certainty_level(calibrated_confidence)

        # Ø­Ø³Ø§Ø¨ Ø¬ÙˆØ¯Ø© Ø§Ù„Ù…Ø¹Ø§ÙŠØ±Ø©
        calibration_quality = self._assess_calibration_quality(factors)

        # ØªÙˆÙ„ÙŠØ¯ ØªÙˆØµÙŠØ©
        recommendation = self._generate_recommendation(calibrated_confidence, certainty_level, factors)

        index = CertaintyIndex(
            overall_confidence=calibrated_confidence,
            certainty_level=certainty_level,
            factors=factors,
            calibration_quality=calibration_quality,
            recommendation=recommendation
        )

        self.total_decisions += 1
        return index

    def _apply_calibration(self, raw_confidence: float, factors: List[ConfidenceFactor]) -> float:
        """
        ØªØ·Ø¨ÙŠÙ‚ Ù…Ø¹Ø§ÙŠØ±Ø© Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©

        ÙŠØ³ØªØ®Ø¯Ù… calibration_errors Ù„ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ø«Ù‚Ø©
        """
        if not self.calibration_errors:
            return raw_confidence

        # Ø­Ø³Ø§Ø¨ Ù…ØªÙˆØ³Ø· Ø®Ø·Ø£ Ø§Ù„Ù…Ø¹Ø§ÙŠØ±Ø©
        avg_error = sum(self.calibration_errors) / len(self.calibration_errors)

        # ØªØ·Ø¨ÙŠÙ‚ ØªØµØ­ÙŠØ­ Ø¨Ø³ÙŠØ·
        # Ø¥Ø°Ø§ ÙƒÙ†Ø§ Ø¯Ø§Ø¦Ù…Ø§Ù‹ Ù†Ø¨Ø§Ù„Øº ÙÙŠ Ø§Ù„Ø«Ù‚Ø© (avg_error > 0)ØŒ Ù†Ø®ÙØ¶ Ø§Ù„Ø«Ù‚Ø©
        # Ø¥Ø°Ø§ ÙƒÙ†Ø§ Ø¯Ø§Ø¦Ù…Ø§Ù‹ Ù†Ù‚Ù„Ù„ Ù…Ù† Ø§Ù„Ø«Ù‚Ø© (avg_error < 0)ØŒ Ù†Ø±ÙØ¹ Ø§Ù„Ø«Ù‚Ø©
        correction = -avg_error * 0.1  # ØªØµØ­ÙŠØ­ Ø¨Ù†Ø³Ø¨Ø© 10%

        calibrated = raw_confidence + correction

        # Clamp to [0, 1]
        return max(0.0, min(1.0, calibrated))

    def _determine_certainty_level(self, confidence: float) -> CertaintyLevel:
        """ØªØ­Ø¯ÙŠØ¯ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ÙŠÙ‚ÙŠÙ†"""
        if confidence >= 0.95:
            return CertaintyLevel.ABSOLUTE
        elif confidence >= 0.85:
            return CertaintyLevel.HIGH
        elif confidence >= 0.70:
            return CertaintyLevel.MODERATE
        elif confidence >= 0.50:
            return CertaintyLevel.LOW
        elif confidence >= 0.30:
            return CertaintyLevel.UNCERTAIN
        else:
            return CertaintyLevel.VERY_UNCERTAIN

    def _assess_calibration_quality(self, factors: List[ConfidenceFactor]) -> float:
        """
        ØªÙ‚ÙŠÙŠÙ… Ø¬ÙˆØ¯Ø© Ø§Ù„Ù…Ø¹Ø§ÙŠØ±Ø©

        ÙƒÙ„Ù…Ø§ Ø²Ø§Ø¯ Ø¹Ø¯Ø¯ Ø§Ù„Ø¹ÙˆØ§Ù…Ù„ Ø§Ù„Ù…ØªÙˆÙØ±Ø©ØŒ Ø²Ø§Ø¯Øª Ø¬ÙˆØ¯Ø© Ø§Ù„Ù…Ø¹Ø§ÙŠØ±Ø©
        """
        available_factors = len(factors)
        total_factors = len(ConfidenceSource)

        coverage = available_factors / total_factors

        # Ø­Ø³Ø§Ø¨ ØªØ¨Ø§ÙŠÙ† Ø§Ù„Ù†Ù‚Ø§Ø· (ÙƒÙ„Ù…Ø§ Ù‚Ù„ Ø§Ù„ØªØ¨Ø§ÙŠÙ†ØŒ Ø²Ø§Ø¯Øª Ø§Ù„Ø¬ÙˆØ¯Ø©)
        if len(factors) >= 2:
            scores = [f.score for f in factors]
            variance = sum((s - sum(scores) / len(scores)) ** 2 for s in scores) / len(scores)
            consistency = 1.0 - min(variance, 1.0)
        else:
            consistency = 0.5

        # Ø¬ÙˆØ¯Ø© Ø§Ù„Ù…Ø¹Ø§ÙŠØ±Ø© = (coverage * 0.6) + (consistency * 0.4)
        quality = (coverage * 0.6) + (consistency * 0.4)

        return quality

    def _generate_recommendation(self,
                                 confidence: float,
                                 level: CertaintyLevel,
                                 factors: List[ConfidenceFactor]) -> str:
        """ØªÙˆÙ„ÙŠØ¯ ØªÙˆØµÙŠØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ÙŠÙ‚ÙŠÙ†"""
        if level == CertaintyLevel.ABSOLUTE:
            return "Proceed with high confidence - decision is well-calibrated"
        elif level == CertaintyLevel.HIGH:
            return "Proceed - confidence is strong"
        elif level == CertaintyLevel.MODERATE:
            return "Proceed with caution - consider validation"
        elif level == CertaintyLevel.LOW:
            return "Review decision - seek additional evidence"
        elif level == CertaintyLevel.UNCERTAIN:
            return "High uncertainty - request clarification or more data"
        else:
            return "Do not proceed - confidence too low, re-evaluate approach"

    def record_outcome(self, predicted_confidence: float, actual_success: bool):
        """
        ØªØ³Ø¬ÙŠÙ„ Ù†ØªÙŠØ¬Ø© Ù‚Ø±Ø§Ø± ÙØ¹Ù„ÙŠ Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù…Ø¹Ø§ÙŠØ±Ø©

        Args:
            predicted_confidence: Ø§Ù„Ø«Ù‚Ø© Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø© (0.0-1.0)
            actual_success: Ù‡Ù„ Ù†Ø¬Ø­ Ø§Ù„Ù‚Ø±Ø§Ø± ÙØ¹Ù„ÙŠØ§Ù‹ØŸ
        """
        actual_confidence = 1.0 if actual_success else 0.0
        error = predicted_confidence - actual_confidence

        self.calibration_errors.append(error)

        # Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ø¢Ø®Ø± 100 Ø®Ø·Ø£ ÙÙ‚Ø·
        if len(self.calibration_errors) > 100:
            self.calibration_errors.pop(0)

        self.decision_history.append({
            "predicted": predicted_confidence,
            "actual": actual_confidence,
            "error": error,
            "timestamp": datetime.now(timezone.utc).isoformat()
        })

    def get_calibration_stats(self) -> Dict[str, Any]:
        """Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù…Ø¹Ø§ÙŠØ±Ø©"""
        if not self.calibration_errors:
            return {
                "total_decisions": self.total_decisions,
                "calibration_quality": "no_data",
                "mean_error": 0.0,
                "rmse": 0.0
            }

        mean_error = sum(self.calibration_errors) / len(self.calibration_errors)
        rmse = math.sqrt(sum(e ** 2 for e in self.calibration_errors) / len(self.calibration_errors))

        # ØªØ­Ø¯ÙŠØ¯ Ø¬ÙˆØ¯Ø© Ø§Ù„Ù…Ø¹Ø§ÙŠØ±Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ RMSE
        if rmse < 0.1:
            quality = "excellent"
        elif rmse < 0.2:
            quality = "good"
        elif rmse < 0.3:
            quality = "acceptable"
        else:
            quality = "needs_improvement"

        return {
            "total_decisions": self.total_decisions,
            "calibration_quality": quality,
            "mean_error": mean_error,
            "rmse": rmse,
            "recent_errors": self.calibration_errors[-10:]
        }

    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # Evidence Generation (helper methods)
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

    def _get_data_quality_evidence(self, score: float) -> str:
        if score >= 0.9:
            return "Data quality: excellent - complete and validated"
        elif score >= 0.7:
            return "Data quality: good - mostly complete"
        elif score >= 0.5:
            return "Data quality: acceptable - some gaps exist"
        else:
            return "Data quality: poor - significant gaps"

    def _get_model_agreement_evidence(self, score: float) -> str:
        if score >= 0.9:
            return "Model agreement: strong consensus across all models"
        elif score >= 0.7:
            return "Model agreement: majority consensus"
        elif score >= 0.5:
            return "Model agreement: partial consensus"
        else:
            return "Model agreement: significant disagreement"

    def _get_historical_accuracy_evidence(self, score: float) -> str:
        if score >= 0.9:
            return "Historical accuracy: excellent track record"
        elif score >= 0.7:
            return "Historical accuracy: good performance"
        elif score >= 0.5:
            return "Historical accuracy: moderate performance"
        else:
            return "Historical accuracy: poor track record"

    def _get_context_clarity_evidence(self, score: float) -> str:
        if score >= 0.9:
            return "Context clarity: fully clear and unambiguous"
        elif score >= 0.7:
            return "Context clarity: mostly clear"
        elif score >= 0.5:
            return "Context clarity: somewhat ambiguous"
        else:
            return "Context clarity: highly ambiguous"

    def _get_reasoning_depth_evidence(self, score: float) -> str:
        if score >= 0.9:
            return "Reasoning depth: comprehensive multi-step analysis"
        elif score >= 0.7:
            return "Reasoning depth: solid analysis"
        elif score >= 0.5:
            return "Reasoning depth: basic analysis"
        else:
            return "Reasoning depth: shallow analysis"

    def _get_cross_validation_evidence(self, score: float) -> str:
        if score >= 0.9:
            return "Cross-validation: verified across multiple sources"
        elif score >= 0.7:
            return "Cross-validation: verified with some sources"
        elif score >= 0.5:
            return "Cross-validation: limited verification"
        else:
            return "Cross-validation: no verification"


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# Usage Example
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

if __name__ == "__main__":
    print("ğŸ¯ Meta-Confidence Calibration System - Test")
    print("=" * 70)
    print()

    calibrator = MetaConfidenceCalibrator()

    # Ù…Ø«Ø§Ù„ 1: Ø«Ù‚Ø© Ø¹Ø§Ù„ÙŠØ© (Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¹ÙˆØ§Ù…Ù„ Ù‚ÙˆÙŠØ©)
    print("1ï¸âƒ£ High Confidence Decision:")
    print("-" * 70)

    index = calibrator.calculate_certainty(
        data_quality=0.95,
        model_agreement=0.90,
        historical_accuracy=0.88,
        context_clarity=0.92,
        reasoning_depth=0.85,
        cross_validation=0.90
    )

    print(f"Overall Confidence: {index.overall_confidence * 100:.1f}%")
    print(f"Certainty Level: {index.certainty_level.value}")
    print(f"Calibration Quality: {index.calibration_quality:.2f}")
    print(f"Recommendation: {index.recommendation}")
    print()

    # Ù…Ø«Ø§Ù„ 2: Ø«Ù‚Ø© Ù…Ù†Ø®ÙØ¶Ø© (Ø¹ÙˆØ§Ù…Ù„ Ø¶Ø¹ÙŠÙØ©)
    print("2ï¸âƒ£ Low Confidence Decision:")
    print("-" * 70)

    index2 = calibrator.calculate_certainty(
        data_quality=0.45,
        model_agreement=0.40,
        context_clarity=0.35
    )

    print(f"Overall Confidence: {index2.overall_confidence * 100:.1f}%")
    print(f"Certainty Level: {index2.certainty_level.value}")
    print(f"Calibration Quality: {index2.calibration_quality:.2f}")
    print(f"Recommendation: {index2.recommendation}")
    print()

    # ØªØ³Ø¬ÙŠÙ„ Ù†ØªØ§Ø¦Ø¬ (Ù„Ù„ØªØ¹Ù„Ù…)
    print("3ï¸âƒ£ Recording Outcomes:")
    print("-" * 70)

    calibrator.record_outcome(predicted_confidence=0.90, actual_success=True)
    calibrator.record_outcome(predicted_confidence=0.85, actual_success=True)
    calibrator.record_outcome(predicted_confidence=0.70, actual_success=False)

    stats = calibrator.get_calibration_stats()
    print(f"Total Decisions: {stats['total_decisions']}")
    print(f"Calibration Quality: {stats['calibration_quality']}")
    print(f"Mean Error: {stats['mean_error']:.3f}")
    print(f"RMSE: {stats['rmse']:.3f}")
    print()

    print("=" * 70)
    print("âœ… Meta-Confidence Calibration System Test Complete!")
    print()
